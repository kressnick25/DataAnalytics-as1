{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39999 entries, 0 to 39998\n",
      "Data columns (total 18 columns):\n",
      "ID                        39999 non-null int64\n",
      "Age                       39032 non-null float64\n",
      "WorkClass                 39027 non-null object\n",
      "Weighting                 38707 non-null float64\n",
      "Education                 39027 non-null object\n",
      "NumYearsEducation         39027 non-null float64\n",
      "MaritalStatus             39027 non-null object\n",
      "Occupation                39013 non-null object\n",
      "Relationship              39027 non-null object\n",
      "Race                      45 non-null object\n",
      "Gender                    39027 non-null object\n",
      "CapitalLoss               39027 non-null float64\n",
      "CapitalGain               39027 non-null float64\n",
      "CapitalAvg                39027 non-null float64\n",
      "NumWorkingHoursPerWeek    39027 non-null float64\n",
      "Sex                       39027 non-null float64\n",
      "Country                   39969 non-null object\n",
      "AtRisk                    39999 non-null object\n",
      "dtypes: float64(8), int64(1), object(9)\n",
      "memory usage: 5.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import dataset\n",
    "df = pd.read_csv('./HouseholderAtRisk.csv')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Data selection and distribution (4 marks)\n",
    "\n",
    "1. What is the proportion of householders who have high risk?\n",
    "\n",
    "**76.246% of householders have high risk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      AtRisk\n",
      "High   30498\n",
      "Low     9501\n",
      "\n",
      " High percentage: 76.24631231561578 %\n"
     ]
    }
   ],
   "source": [
    "print(df['AtRisk'].value_counts().to_frame())\n",
    "\n",
    "total = 30497 + 9501\n",
    "print('\\n High percentage:', (30497 / total) * 100, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Did you have to fix any data quality problems?\n",
    "\n",
    "Yes US, USA United states in country have all been compacted into one value in country\n",
    "\n",
    "Values such as '?', Undefined and any NaN's all needed to be removed in replaced with a value\n",
    "\n",
    "Yes some rows were filled with entirely null values. Rows with only 3-4 values or 14 to 18 nulls where all removed. This step actually appears to remove almost all columns with null values. It can be seen that most rows have about 1 null value this appears to be coming from the race column which will be dropped later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                            0\n",
      "Age                         967\n",
      "WorkClass                   972\n",
      "Weighting                  1292\n",
      "Education                   972\n",
      "NumYearsEducation           972\n",
      "MaritalStatus               972\n",
      "Occupation                  986\n",
      "Relationship                972\n",
      "Race                      39954\n",
      "Gender                      972\n",
      "CapitalLoss                 972\n",
      "CapitalGain                 972\n",
      "CapitalAvg                  972\n",
      "NumWorkingHoursPerWeek      972\n",
      "Sex                         972\n",
      "Country                      30\n",
      "AtRisk                        0\n",
      "dtype: int64\n",
      "1     38648\n",
      "15      947\n",
      "2       334\n",
      "0        45\n",
      "16       25\n",
      "Name: full_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum()) # Count of Null in columns\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df['full_count'] = df.apply(lambda x: 18-x.count(), axis=1)\n",
    "print(test_df['full_count'].value_counts()) # Count of nulls in rows (null values: Number of rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    38648\n",
      "2.0      334\n",
      "0.0       45\n",
      "Name: full_count, dtype: int64\n",
      "0.0    39027\n",
      "Name: full_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove extra white spaces\n",
    "df_obj = df.select_dtypes(['object'])\n",
    "df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "\n",
    "df['full_count'] = df.apply(lambda x: 18-x.count(), axis=1)\n",
    "df.drop(df[df['full_count'] >= 14].index, inplace=True)\n",
    "df = df.drop(columns='full_count') # Remove count of null values in rows as they are no longer needed for data calc\n",
    "\n",
    "df['Country'].replace(\"USA\", \"United-States\", inplace=True)\n",
    "df['Country'].replace(\"US\", \"United-States\", inplace=True)\n",
    "df['Country'].replace(\"?\", df['Country'].value_counts().idxmax(), inplace=True)\n",
    "df['Occupation'].replace(\"?\", df['Occupation'].value_counts().idxmax(), inplace=True)\n",
    "df['WorkClass'].replace(\"?\", df['WorkClass'].value_counts().idxmax(), inplace=True)\n",
    "\n",
    "test_df['full_count'] = df.apply(lambda x: 18-x.count(), axis=1)\n",
    "print(test_df['full_count'].value_counts()) \n",
    "\n",
    "\n",
    "df = df.apply(lambda x: x.fillna(x.value_counts().idxmax())) # Remove\n",
    "\n",
    "test_df['full_count'] = df.apply(lambda x: 18-x.count(), axis=1)\n",
    "print(test_df['full_count'].value_counts()) # Count of nulls in rows (null values: Number of rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The dataset may include irrelevant and redundant variables. What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "\n",
    "Race is practly empty\n",
    "Sex is redundent and gender will be used instead\n",
    "ID is irrelevent giving no unique information\n",
    "Education is redunent years of education can be used instead\n",
    "Capital average is redundent being derived from (capital loss + capital gain) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Race'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-78b64233adf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Race'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Race'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3940\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3778\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3779\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3780\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3782\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3811\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3812\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4964\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4965\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4967\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Race'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Clean data to be prepared for the decision tree\n",
    "df = df.drop(columns='CapitalAvg')\n",
    "df = df.drop(columns='Sex')\n",
    "df = df.drop(columns='Education')\n",
    "df = df.drop(columns='Race')\n",
    "df = df.drop(columns='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What distribution scheme did you use? What “data partitioning allocation” did you set? Explain your selection. (Hint: Take the lead from Week 2 lecture on data distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df['AtRisk'],AtRisk = pd.factorize(df['AtRisk'])\n",
    "df['Country'],Country = pd.factorize(df['Country'])\n",
    "df['Gender'],Gender = pd.factorize(df['Gender'])\n",
    "df['WorkClass'],WorkClass = pd.factorize(df['WorkClass'])\n",
    "df['MaritalStatus'],AtRisk = pd.factorize(df['MaritalStatus'])\n",
    "df['Occupation'],AtRisk = pd.factorize(df['Occupation'])\n",
    "df['Relationship'],AtRisk = pd.factorize(df['Relationship'])\n",
    "\n",
    "\n",
    "y = df['AtRisk']\n",
    "x = df.drop(['AtRisk'], axis=1).as_matrix()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Predictive Modelling Using Decision Trees (4 marks)\n",
    "1. Build a decision tree using the default setting. Examine the tree results and answer the followings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9999267881982575\n",
      "Test accuracy: 0.8160389444017423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(x_train, y_train))\n",
    "x_test = np.where(np.isfinite(x_test)==False, 0, x_test)\n",
    "print(\"Test accuracy:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.241527 to fit\r\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph.write_png(\"viz.png\") # saved in the following file - will return True if successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  a. What is classification accuracy on training and test datasets?\n",
    "  \n",
    "  b. Which variable is used for the first split? What are the variables that are used for the second split?\n",
    "  \n",
    "  c. What are the 5 important variables in building the tree?\n",
    "  \n",
    "  d. Report if you see any evidence of model overfitting.\n",
    "  \n",
    "  \n",
    "2. Build another decision tree tuned with GridSearchCV. Examine the tree results.\n",
    "  \n",
    "  a. What is classification accuracy on training and test datasets?\n",
    "  \n",
    "  b. What are the parameters used? Explain your decision.\n",
    "  \n",
    "  c. What are the optimal parameters for this decision tree?\n",
    "  \n",
    "  d. Which variable is used for the first split? What are the variables that are used for the second split?\n",
    "  \n",
    "  e. What are the 5 important variables in building the tree?\n",
    "  \n",
    "  f. Report if you see any evidence of model overfitting.\n",
    "  \n",
    "  \n",
    "3. What is the significant difference do you see between these two decision tree models – default (Task 2.1) and using GridSearchCV (Task 2.2)? How do theycompare performance-wise? Explain why those changes may have happened.\n",
    "\n",
    "\n",
    "4. From the better model, can you identify which householders to target for providing loan? Can you provide some descriptive summary of those householders?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "## Predictive Modeling Using Regression (5.5 marks)\n",
    "1. Describe why you will have to do additional preparation for variables to be\n",
    "used in regression modelling. Apply transformation method(s) to the\n",
    "variable(s) that need it. List the variables that needed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "print(\"Train accuracy:\", logisticRegr.score(x_train, y_train))\n",
    "x_test = np.where(np.isfinite(x_test)==False, 0, x_test)\n",
    "print(\"Test accuracy:\", logisticRegr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build a regression model using the default regression method with all\n",
    "inputs. Once you have completed it, build another model and tune it usingGridSearchCV. Answer the followings:\n",
    "a. Report which variables are included in the regression model.\n",
    "b. Report the top-5 important variables (in the order) in the model.\n",
    "c. Report any sign of overfitting.\n",
    "d. What are the parameters used? Explain your decision. What are the\n",
    "optimal parameters? Which regression function is being used?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "3. Build another regression model using the subset of inputs selected either\n",
    "by RFE or the selection by model method. Answer the followings:\n",
    "a. Report which variables are included in the regression model.\n",
    "b. Report the top-5 important variables (in the order) in the model.\n",
    "c. Report any sign of overfitting.\n",
    "d. What is classification accuracy on training and test datasets?\n",
    "4. Using the comparison statistics, which of the regression models appears to\n",
    "be better? Is there any difference between the two models (i.e one with\n",
    "selected variables and another with all variables)? Explain why those\n",
    "changes may have happened.\n",
    "5. From the better model, can you identify which householders to target for\n",
    "providing loan? Can you provide some descriptive summary of those\n",
    "householders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## Predictive Modeling Using Neural Networks (5.5 marks)\n",
    "1. Build a Neural Network model using the default setting. Answer the\n",
    "following:\n",
    "a. What are the parameters used? Explain your decision. What is the\n",
    "network architecture?\n",
    "b. How many iterations are needed to train this network?\n",
    "c. Do you see any sign of over-fitting?\n",
    "d. Did the training process converge and resulted in the best model?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "2. Refine this network by tuning it with GridSearchCV. Answer the\n",
    "following:\n",
    "a. What are the parameters used? Explain your decision. What is the\n",
    "network architecture?\n",
    "b. How many iterations are needed to train this network?\n",
    "c. Do you see any sign of over-fitting?\n",
    "d. Did the training process converge and resulted in the best model?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "3. Would feature selection help here? Build another Neural Network model\n",
    "with inputs selected from RFE with regression (use the best model\n",
    "generated in Task 3) and from the decision tree (use the best model\n",
    "from Task 2). Answer the following for the best neural network model:a. Did feature selection help here? Which method of feature selection\n",
    "produced the best result? Any change in the network architecture?\n",
    "What inputs are being used as the network input?\n",
    "b. What is classification accuracy on training and test datasets? Is there\n",
    "any improvement in the outcome?\n",
    "c. How many iterations are now needed to train this network?\n",
    "d. Do you see any sign of over-fitting?\n",
    "e. Did the training process converge and resulted in the best model?\n",
    "f. Finally, see whether the change in network architecture can further\n",
    "improve the performance, use GridSearchCV to tune the network.\n",
    "Report if there was any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2))\n",
    "nn.fit(x_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", nn.score(x_train, y_train))\n",
    "x_test = np.where(np.isfinite(x_test)==False, 0, x_test)\n",
    "print(\"Test accuracy:\", nn.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "## Comparing Predictive Models (4 marks)\n",
    "1. Use the comparison methods to compare the best decision tree model, the\n",
    "best regression model, and the best neural network model.\n",
    "a. Discuss the findings led by:\n",
    "(i) ROC Chart and Index;\n",
    "(ii) Accuracy Score;\n",
    "b. Which model would you use in deployment based on these findings?\n",
    "Discuss why?\n",
    "c. Do all the models agree on the householder’s characteristics? How do\n",
    "they vary?\n",
    "2. How the outcome of this study can be used by decision makers?\n",
    "3. Can you summarise the positives and negative aspects of each predictive\n",
    "modelling method based on this data analysis exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
