{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pydot\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "df = pd.read_csv('./HouseholderAtRisk.csv')\n",
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=20):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "    \n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    for i in indices:\n",
    "        print(feature_names[i] + ': ' + str(importances[i]))\n",
    "        \n",
    "def visualize_decision_tree(dm_model, feature_names, save_name):\n",
    "    dotfile = BytesIO()\n",
    "    export_graphviz(dm_model, out_file=dotfile, feature_names=feature_names)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png(save_name) # saved in the following file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 1</h2>\n",
    "<h3>1. What is the proportion of householders at risk?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High    30498\n",
      "Low      9501\n",
      "Name: AtRisk, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['AtRisk'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of 39999 observations, we found\n",
    "High Risk : 30498 (76.247%)\n",
    "Low Risk: 9501 (23.753%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.Did you have to fix any data quality problems?</h3>\n",
    "\n",
    "Missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                            0\n",
      "Age                         967\n",
      "WorkClass                   972\n",
      "Weighting                  1292\n",
      "Education                   972\n",
      "NumYearsEducation           972\n",
      "MaritalStatus               972\n",
      "Occupation                  986\n",
      "Relationship                972\n",
      "Race                      39954\n",
      "Gender                      972\n",
      "CapitalLoss                 972\n",
      "CapitalGain                 972\n",
      "CapitalAvg                  972\n",
      "NumWorkingHoursPerWeek      972\n",
      "Sex                         972\n",
      "Country                      30\n",
      "AtRisk                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attributes had quality problems:\n",
    "<h5>Race</h5>\n",
    "Most values in this attribute were missing.\n",
    "The fix was to drop this column.\n",
    "<h5>Age</h5>\n",
    "There were a few values less than 1 and there were 968 missing values.\n",
    "These values were imputed with the mean value of 38.66.\n",
    "<h5>WorkClass</h5>\n",
    "The values were prepended with a space. The space was removed .\n",
    "There were 2240 records with invalid value of \"?\". There were 972 missing values. These values were imputed with the mode \"Private\".\n",
    "<h5>NumYearsEducation</h5>\n",
    "There were 972 missing values. These values were replaced with the mean value of 10.\n",
    "<h5>MaritalStatus</h5>\n",
    "The values in this attribute were prepended with a space. The space was removed.\n",
    "\n",
    "There were 972 missing values. These were replaced with the mode \"Married-civ-spouse\".\n",
    "<h5>Occupation</h5>\n",
    "The values in this attribute were prepended with a space. The space was removed.\n",
    "\n",
    "There were 2246 records with invalid value of \"?\" and there were 986 missing values. These values were imputed with the mode \"Prof-specialty\".\n",
    "<h5>Relationship</h5>\n",
    "The values in this attribute were prepended with a space. The space was removed.\n",
    "\n",
    "The 972 missing values were imputed with ‘Husband’ which is the mode. \n",
    "<h5>CapitalLoss</h5>\n",
    "The 972 missing values were imputed with the mode of 0. Considering the values being skewed to the far left, it makes sense to impute 0 to the missing values.\n",
    "<h5>CapitalGain</h5>\n",
    "The 972 missing values were imputed with the mode of 0.\n",
    "<h5>CapitalAvg</h5>\n",
    "The 972 missing values were imputed with the mode of 0.\n",
    "<h5>NumWorkingHoursPerWeek</h5>\n",
    "There were 972 missing values. These values were imputed with the mean value of 40.\n",
    "<h5>Sex</h5>\n",
    "There were 972 missing values. These values were imputed with the mode of 0.\n",
    "<h5>Country</h5>\n",
    "The values in this attribute were prepended with a space. The space was removed.\n",
    "\n",
    "<p>699 values were ‘?’ - These were imputed with the mode‘United-States’.</p>\n",
    "<p>30 missing values were imputed with ‘United-States’</p>\n",
    "<p>917 values were ‘USA’ - These were changed to ‘United-States’</p>\n",
    "<p>9 values were ‘US’ - These were changed to ‘United-States’</p>\n",
    "<p>20 values were ‘Hong’ - These were changed to ‘United-States’</p>\n",
    "<p>97 values were South - These were imputed with 'United-States'</p>\n",
    "\n",
    "<h3>Data types</h3>\n",
    "<h5>Age</h5>\n",
    "The data type was converted from float to int.\n",
    "<h5>Sex</h5>\n",
    "The data type was converted from float to binary.\n",
    "<h5>NumYearsEducation</h5>\n",
    "The data type was converted from float to int.\n",
    "<h5>Weighting</h5>\n",
    "The data type was converted from float to int.\n",
    "<h5>AtRisk</h5>\n",
    "There are only two possible values 'High' or 'Low'. This can be formatted as binary variable.\n",
    "\n",
    "<h3>One-Hot Encoding</h3>\n",
    "The following categorical variables needs to be converted to numerical variables\n",
    "<h5>Country</h5>\n",
    "<h5>MaritalStatus</h5>\n",
    "<h5>Occupation</h5>\n",
    "<h5>Relationship</h5>\n",
    "<h5>Country</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Irrelevant and redundant variables</h3>\n",
    "\n",
    "<h5>ID</h5>\n",
    "This attribute is a unique identifier and does not provide useful information for predicting the target variable.\n",
    "<h5>Gender</h5>\n",
    "This attribute is identical to Sex attribute but with different name. Sex attribute was chosen over this because when there are only two possible values it is better to transform it to binary variable. \n",
    "<h5>Education</h5>\n",
    "Education attribute and NumYearsEducation is essentially a one-to-one mapping except that Education attribute is ordinal but NumYearsEducation is numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID, Weighting, Race, Gender, Education\n",
    "df.drop(['ID', 'Race', 'Gender', 'Education'], axis=1, inplace=True)\n",
    "\n",
    "### Age Column\n",
    "# Age less than 1 is invalid\n",
    "# Impute the invalid values and missing values with mean\n",
    "# because ...\n",
    "mask = df['Age'] < 1\n",
    "df.loc[mask, 'Age'] = np.nan\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "\n",
    "### WorkClass column\n",
    "# Remove spaces\n",
    "for uniq in df['WorkClass'].unique():\n",
    "    if isinstance(uniq, str):\n",
    "        mask = df['WorkClass'] == uniq\n",
    "        df.loc[mask, 'WorkClass'] = uniq[1:]\n",
    "\n",
    "mask = df['WorkClass'] == '?'\n",
    "df.loc[mask, 'WorkClass'] = np.nan\n",
    "df['WorkClass'].fillna('Private', inplace=True)\n",
    "\n",
    "### Weighting column\n",
    "df['Weighting'].fillna(df['Weighting'].mean(), inplace=True)\n",
    "\n",
    "### NumYearsEducation column\n",
    "df['NumYearsEducation'].fillna(df['NumYearsEducation'].mean(), inplace=True)\n",
    "\n",
    "### MaritalStatus column\n",
    "# Remove spaces\n",
    "for uniq in df['MaritalStatus'].unique():\n",
    "    if isinstance(uniq, str):\n",
    "        mask = df['MaritalStatus'] == uniq\n",
    "        df.loc[mask, 'MaritalStatus'] = uniq[1:]\n",
    "\n",
    "df['MaritalStatus'].fillna('Married-civ-spouse', inplace=True)\n",
    "\n",
    "### Occupation column\n",
    "for uniq in df['Occupation'].unique():\n",
    "    if isinstance(uniq, str):\n",
    "        mask = df['Occupation'] == uniq\n",
    "        df.loc[mask, 'Occupation'] = uniq[1:]\n",
    "\n",
    "mask = df['Occupation'] == '?'\n",
    "df.loc[mask, 'Occupation'] = np.nan\n",
    "df['Occupation'].fillna('Prof-specialty', inplace=True)\n",
    "\n",
    "### Relationship column\n",
    "# Remove spaces\n",
    "for uniq in df['Relationship'].unique():\n",
    "    if isinstance(uniq, str):\n",
    "        mask = df['Relationship'] == uniq\n",
    "        df.loc[mask, 'Relationship'] = uniq[1:]\n",
    "\n",
    "df['Relationship'].fillna('Husband', inplace=True)\n",
    "\n",
    "### CapitalLoss column\n",
    "# Impute missing values with 0 which is the median\n",
    "# because the data has great outliers (Skewed to left)\n",
    "df['CapitalLoss'].fillna(0, inplace=True)\n",
    "\n",
    "### CapitalGain column\n",
    "# Impute missing values with 0\n",
    "df['CapitalGain'].fillna(0, inplace=True)\n",
    "\n",
    "### CapitalAvg column\n",
    "# Impute with 0\n",
    "df['CapitalAvg'].fillna(0, inplace=True)\n",
    "\n",
    "### NumWorkingHoursPerWeek column\n",
    "# Impute with mean of 40\n",
    "df['NumWorkingHoursPerWeek'].fillna(df['NumWorkingHoursPerWeek'].mean(), inplace=True)\n",
    "\n",
    "### Sex column\n",
    "# Impute with 0 which is the mode\n",
    "df['Sex'].fillna(0, inplace=True)\n",
    "\n",
    "### Country column\n",
    "# Remove spaces \n",
    "for uniq in df['Country'].unique():\n",
    "    if isinstance(uniq, str):\n",
    "        mask = df['Country'] == uniq\n",
    "        df.loc[mask, 'Country'] = uniq[1:]\n",
    "\n",
    "mask = df['Country'] == '?'\n",
    "df.loc[mask, 'Country'] = 'United-States'\n",
    "mask = df['Country'] == 'USA'\n",
    "df.loc[mask, 'Country'] = 'United-States'\n",
    "mask = df['Country'] == 'US'\n",
    "df.loc[mask, 'Country'] = 'United-States'\n",
    "mask = df['Country'] == 'Hong'\n",
    "df.loc[mask, 'Country'] = 'United-States'\n",
    "mask = df['Country'] == 'South'\n",
    "df.loc[mask, 'Country'] = 'United-States'\n",
    "df['Country'].fillna('United-States', inplace=True)\n",
    "\n",
    "### Data types\n",
    "# format Sex to binary\n",
    "data_type_map = {1.0: 1, 0.0: 0}\n",
    "df['Sex'] = df['Sex'].map(data_type_map)\n",
    "# format Age to int\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "# # format NumYearsEducation to int\n",
    "df['NumYearsEducation'] = df['NumYearsEducation'].astype(int)\n",
    "# format Weighting to int\n",
    "df['Weighting'] = df['Weighting'].astype(int)\n",
    "# # format AtRisk to binary\n",
    "data_type_map = {'High': 1, 'Low': 0}\n",
    "df['AtRisk'] = df['AtRisk'].map(data_type_map)\n",
    "\n",
    "\n",
    "### One-Hot Encoding\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. What distribution scheme did you use? What data partitioning allocation did you set?</h3>\n",
    "I used 70/30 split (Test dataset is 30%) with stratified sampling. I used stratified sampling because our dataset is skewed (76% of instances are high risk). Using random sampling can produce an inaccurate or overfitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "y = df['AtRisk']\n",
    "x = df.drop(['AtRisk'], axis=1)\n",
    "\n",
    "rs = 20\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "x_mat = x.as_matrix()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 2. Decision Trees</h2>\n",
    "<h3>1. Build a decision tree using the default setting. Examine the tree results and answer the followings</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=20,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=rs)\n",
    "dt_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=20,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "print(dt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a. What is classification accuracy on training and test datasets?</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy on training dataset: ', 0.9942855101967928)\n",
      "('Accuracy on test dataset: ', 0.8155)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training dataset: \", dt_model.score(x_train, y_train))\n",
    "print(\"Accuracy on test dataset: \", dt_model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training Dataset</b>: 99.42% accuracy<br/>\n",
    "<b>Test Dataset</b>: 81.55% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>b. Which variable is used for the first split? What are the variables used for the second split?</h5>\n",
    "The graph image wasn't visible..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>c. What are the 5 important variables in building the tree?</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaritalStatus_Married-civ-spouse: 0.19643803367866805\n",
      "Weighting: 0.1821976283210954\n",
      "NumYearsEducation: 0.12380858977358129\n",
      "Age: 0.1231522015036295\n",
      "CapitalAvg: 0.07077221883690106\n"
     ]
    }
   ],
   "source": [
    "analyse_feature_importance(dt_model, x.columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 important variable are:<br/>\n",
    "MaritalStatus_Married-civ-spouse: 0.19643803367866805<br/>\n",
    "Weighting: 0.1821976283210954<br/>\n",
    "NumYearsEducation: 0.12380858977358129<br/>\n",
    "Age: 0.1231522015036295<br/>\n",
    "CapitalAvg: 0.07077221883690106<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_decision_tree(dt_model, x.columns, \"./graph1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>d. Report if you see any evidence of model overfitting</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is 17.87% higher for training dataset. This is an evidence of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Build another decision tree tuned with GridSearchCV</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=20,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [2, 3, 4, 5, 6], 'min_samples_leaf': [20, 30, 40, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 7), \n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "dt_model_cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "dt_model_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a. What is classification accuracy on training and test datasets?</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.8555305546626665)\n",
      "('Test accuracy:', 0.854)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", dt_model_cv.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", dt_model_cv.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training Dataset</b>: 85.55% accuracy<br/>\n",
    "<b>Test Dataset</b>: 85.4% accuracy<br/>\n",
    "<h5>b. What are the parameters used? Explain your decision.</h5>\n",
    "The hyperparameters used were:\n",
    "<ul>\n",
    "    <li><b>max_depth</b>: To pre-prune the maximal tree. By limiting the maximum depth, we can limit the size of the tree and therefore overfitting</li>\n",
    "    <li><b>min_samples_leaf</b>: Setting larger value for this parameter has similar effect as setting max_depth. It limits the granularity of the tree and reduce overfitting</li>\n",
    "</ul>\n",
    "<h5>What are the optimal parameters for this decision tree?</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 20}\n"
     ]
    }
   ],
   "source": [
    "print(dt_model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters of the cross validation before optimization are:<br/>\n",
    "max_depth: 6<br/>\n",
    "min_samples_leaf: 30<br/>\n",
    "To optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=20,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'max_depth': [4, 5, 6, 7], 'min_samples_leaf': [25, 26, 27, 28, 29, 30, 31, 32, 33, 34]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(4, 8), \n",
    "          'min_samples_leaf': range(25, 35)}\n",
    "\n",
    "dt_model_cv_best = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "dt_model_cv_best.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.8589592485445908)\n",
      "('Test accuracy:', 0.8555833333333334)\n",
      "{'criterion': 'gini', 'max_depth': 7, 'min_samples_leaf': 27}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", dt_model_cv_best.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", dt_model_cv_best.score(x_test, y_test))\n",
    "print(dt_model_cv_best.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training Dataset</b>: 85.9% accuracy<br/>\n",
    "<b>Test Dataset</b>: 85.56% accuracy<br/>\n",
    "\n",
    "The optimal parameters are:\n",
    "max_depth: 7<br/>\n",
    "min_samples_leaf: 27<br/>\n",
    "<h5>d. Which variable is used for the first split? What are the \n",
    "variables that are used for the second split?</h5>\n",
    "<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_decision_tree(dt_model_cv_best.best_estimator_, x.columns, \"./graph2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./graph2.png\"/>\n",
    "<b>First split</b>: MaritalStatus_Married-civ-spouse<br/>\n",
    "<b>Second split</b>: CapitalAvg, NumYearsEducation\n",
    "<h5>e. What are the 5 important variables in building the tree? </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaritalStatus_Married-civ-spouse: 0.426884400135397\n",
      "NumYearsEducation: 0.22390731862508764\n",
      "CapitalAvg: 0.12609972981192208\n",
      "CapitalGain: 0.07810651557643462\n",
      "CapitalLoss: 0.06413572140454132\n"
     ]
    }
   ],
   "source": [
    "analyse_feature_importance(dt_model_cv.best_estimator_, x.columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 important variables are:<br/>\n",
    "MaritalStatus_Married-civ-spouse: 0.4268844001353971<br/>\n",
    "NumYearsEducation: 0.22390731862508767<br/>\n",
    "CapitalAvg: 0.12609972981192208<br/>\n",
    "CapitalGain: 0.07810651557643462<br/>\n",
    "CapitalLoss: 0.06413572140454132<br/>\n",
    "<h5>f. Report if you see any evidence of model overfitting</h5>\n",
    "The accuracy of the model on the training dataset and test dataset is almost the same. Therefore I can say there's no overfitting.\n",
    "<h3>3. What is the significant difference do you see between these two decision tree \n",
    "models</h3>\n",
    "<h5>Performance Difference</h5>\n",
    "The improvement in the decision tree using grid search cross validation was elimination of overfitting and improvement in accuracy in test dataset by 4.35%.\n",
    "This improvement is due to cross validation and tree pruning to reduce overfitting on training data and making the model more generalized. \n",
    "<h5>Other Changes</h5>\n",
    "<p>- depth:<br/>\n",
    "The depth of the latter model was 7 and the depth of the previous model was ...\n",
    "</p>\n",
    "<p>- feature importance:<br/>\n",
    "Both models have MaritalStatus_Married-civ-spouse as their most important variable. However the variable's importance was 0.1964 in the previous model and 0.4269 in the latter model. This means the latter model's splitting is much better than the previous model. This may be the result of cross validation and tree pruning.\n",
    "</p>\n",
    "<h3>4.From the better model, can you identify which householders to target for providing loan? Can you provide some descriptive summary of those householders?</h3>\n",
    "Based on the feature importance and the value split in each node, the following criteria provide the purest split. \n",
    "<p>The householder should meet the following criteria:<br/>\n",
    "MaritalStatus_Married-civ-spouse > 0.5<br/>\n",
    "NumYearsEducation > 11.5<br/>\n",
    "CapitalAvg > 2547.75<br/>\n",
    "WorkClass_Self-emp-not-inc <= 0.5\n",
    "</p>\n",
    "<br/>\n",
    "\n",
    "This means the householder must be married, have a very high level of education (more than 11.5 years),have a significant amount of investment (CapitalAvg of more than 2547.75), and not self employed to have a very low chance of being at risk.\n",
    "<h2>Task 3</h2>\n",
    "<h3>1. Describe why you will have to do additional preparation for variables to be used in regression modelling. Apply transformation methods to the variables that need.</h3>\n",
    "The transformations that needs to be performed on the dataset are standardisation and logarithmic transformation. Our dataset requires standardisation because the input variables are of different scales. For example, Age attribute ranges from 17 to 90 whereas CapitalGain attribute ranges from 0 to 99999. The differences in the scale can affect model performance. Some of the attributes in our dataset requires logarithmic transformation. Logarithmic transformation is used to modify the distribution of input values. Skewed attribute with extreme values can negatively affect model performance.\n",
    "\n",
    "<img src=\"./skew.png\"/>\n",
    "The Age, Weighting, CapitalGain and CapitalAvg attributes have different level of skewedness with Age being the least skewed.\n",
    "<h5>Logarithmic Transformation</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_transform = ['Age', 'Weighting','CapitalGain', 'CapitalAvg']\n",
    "df_log = df.copy()\n",
    "for col in columns_to_transform:\n",
    "    df_log[col] = df_log[col].apply(lambda x: x+1)\n",
    "    df_log[col] = df_log[col].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After logarithmic transformation, the attributes' value distribution looks like:<br/>\n",
    "<img src=\"./skew2.png\"/>\n",
    "\n",
    "<h5>Partitioning data again using the df_log.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = df_log['AtRisk']\n",
    "x_log = df_log.drop(['AtRisk'], axis=1)\n",
    "x_mat_log = x_log.as_matrix()\n",
    "x_train_log, x_test_log, y_train_log, y_test_log = train_test_split(x_mat_log, y_log, test_size=0.3, stratify=y_log, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Standardisation</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_log_scaled = scaler.fit_transform(x_train_log, y_train_log)\n",
    "x_test_log_scaled = scaler.transform(x_test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Build a regression model using the default regression method with all inputs. Once you have completed it, build another model ad tune it using GridSearchCV.</h3>\n",
    "<h5>Building Logistic Regression model</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.843315832708311)\n",
      "('Test accuracy:', 0.8444166666666667)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "reg_model = LogisticRegression(random_state=rs)\n",
    "reg_model.fit(x_train_log_scaled, y_train_log)\n",
    "print(\n",
    "    \"Train accuracy:\", \n",
    "    reg_model.score(x_train_log_scaled, y_train_log)\n",
    ")\n",
    "print(\n",
    "    \"Test accuracy:\", \n",
    "    reg_model.score(x_test_log_scaled, y_test_log)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy: 0.843315832708311<br/>\n",
    "Test Accuracy: 0.8444166666666667<br/>\n",
    "<h5>Building model using GridSearchCV</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.843315832708311)\n",
      "('Test accuracy:', 0.8446666666666667)\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "reg_model_cv = GridSearchCV(\n",
    "    param_grid=params, \n",
    "    estimator=LogisticRegression(random_state=rs),\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "reg_model_cv.fit(x_train_log_scaled, y_train_log)\n",
    "print(\n",
    "    \"Train accuracy:\", \n",
    "    reg_model_cv.score(x_train_log_scaled, y_train_log)\n",
    ")\n",
    "print(\n",
    "    \"Test accuracy:\", \n",
    "    reg_model_cv.score(x_test_log_scaled, y_test_log)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(reg_model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Feature importance</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NumYearsEducation', ':', -0.7713043790777583)\n",
      "('CapitalGain', ':', -0.7094696357669318)\n",
      "('MaritalStatus_Married-civ-spouse', ':', -0.6824205759493448)\n",
      "('MaritalStatus_Never-married', ':', 0.47492015857914405)\n",
      "('CapitalLoss', ':', -0.42211194424844306)\n",
      "('Age', ':', -0.40746014601360725)\n",
      "('NumWorkingHoursPerWeek', ':', -0.3796592659395903)\n",
      "('Sex', ':', 0.3349785382501851)\n",
      "('CapitalAvg', ':', 0.27607410795263676)\n",
      "('Relationship_Wife', ':', -0.2450854445722089)\n",
      "('MaritalStatus_Divorced', ':', 0.24396776829326736)\n",
      "('Occupation_Other-service', ':', 0.24007676387416124)\n",
      "('Relationship_Own-child', ':', 0.23515876490039334)\n",
      "('Occupation_Exec-managerial', ':', -0.2256548257699069)\n",
      "('Occupation_Farming-fishing', ':', 0.14719458551118833)\n",
      "('Relationship_Not-in-family', ':', -0.14465454696683633)\n",
      "('Occupation_Handlers-cleaners', ':', 0.13612927717624357)\n",
      "('Country_Guatemala', ':', 0.13276280580552455)\n",
      "('Relationship_Other-relative', ':', 0.11423285273168488)\n",
      "('Occupation_Priv-house-serv', ':', 0.11074420178137662)\n"
     ]
    }
   ],
   "source": [
    "feature_names = x_log.columns\n",
    "coef = reg_model_cv.best_estimator_.coef_[0]\n",
    "\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a. Report which variables are included in the regression model</h5>\n",
    "All 83 input variables are included in the model.\n",
    "<h5>b. Report the top-5 important variables in the model</h5>\n",
    "<p>\n",
    "NumYearsEducation: -0.7713043790777583<br/>\n",
    "CapitalGain: -0.7094696357669318<br/>\n",
    "MaritalStatus_Married-civ-spouse: -0.6824205759493448<br/>\n",
    "MaritalStatus_Never-married: 0.47492015857914405<br/>\n",
    "CapitalLoss: -0.42211194424844306\n",
    "</p>\n",
    "<h5>c. Report any sign of overfitting</h5>\n",
    "The training accuracy and test accuracy of the model was almost identical. This means there is almost no overfitting.\n",
    "<h5>d. What are the parameters used? Explain your decision. What are the optimal parameters? Which regression function is being used?</h5>\n",
    "The parameter used was the regularisation strength represented as C. The range of values used for this parameter was [pow(10, x) for x in range(-6, 4)]. The regularisation strength was used to reduce overfitting.\n",
    "<p>\n",
    "The optimal parameter is C: 0.1\n",
    "</p>\n",
    "\n",
    "<h5>e. What is classification accuracy on training and test datasets?</h5>\n",
    "<p>\n",
    "The accuracy of model built using the default regression method was a follows:<br/>\n",
    "Train Accuracy: 0.843315832708311<br/>\n",
    "Test Accuracy: 0.8444166666666667<br/>\n",
    "</p>\n",
    "<p>\n",
    "The accuracy of model built using GridSearchCV was a follows:<br/>\n",
    "Train accuracy: 0.843315832708311<br/>\n",
    "Test accuracy: 0.8446666666666667\n",
    "</p>\n",
    "<h3>3. Build another regression model using the subset of inputs selected either by RFE or the selection by model method</h3>\n",
    "<h5>Using recursive feature elimination</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Original feature set', 82)\n",
      "('Number of features after elimination', 44)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe = RFECV(\n",
    "    estimator = LogisticRegression(random_state=rs), \n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfe.fit(x_train_log_scaled, y_train_log) # run the RFECV\n",
    "\n",
    "# comparing how many variables before and after\n",
    "print(\"Original feature set\", x_train_log_scaled.shape[1])\n",
    "print(\"Number of features after elimination\", rfe.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original feature set: 82<br/>\n",
    "Number of features after elimination: 44<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.843315832708311)\n",
      "('Test accuracy:', 0.8455)\n"
     ]
    }
   ],
   "source": [
    "x_train_log_scaled_sel = rfe.transform(x_train_log_scaled)\n",
    "x_test_log_scaled_sel = rfe.transform(x_test_log_scaled)\n",
    "\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "reg_model_cv_rfe = GridSearchCV(\n",
    "    param_grid=params, \n",
    "    estimator=LogisticRegression(random_state=rs),\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "reg_model_cv_rfe.fit(x_train_log_scaled_sel, y_train_log)\n",
    "print(\n",
    "    \"Train accuracy:\", \n",
    "    reg_model_cv_rfe.score(x_train_log_scaled_sel, y_train_log)\n",
    ")\n",
    "print(\n",
    "    \"Test accuracy:\", \n",
    "    reg_model_cv_rfe.score(x_test_log_scaled_sel, y_test_log)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy: 0.843315832708311<br/>\n",
    "Test accuracy: 0.8455<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NumYearsEducation', ':', -0.7084501674400273)\n",
      "('WorkClass_State-gov', ':', -0.4605340082385802)\n",
      "('Age', ':', -0.36480469670520127)\n",
      "('MaritalStatus_Divorced', ':', 0.35113976967417915)\n",
      "('NumWorkingHoursPerWeek', ':', -0.3475917918739649)\n",
      "('CapitalGain', ':', -0.3289549347843966)\n",
      "('Occupation_Protective-serv', ':', -0.3129291628978554)\n",
      "('Sex', ':', 0.28529889029523775)\n",
      "('Occupation_Craft-repair', ':', 0.25196224284596136)\n",
      "('Occupation_Machine-op-inspct', ':', -0.22020808442387893)\n",
      "('WorkClass_Self-emp-not-inc', ':', 0.17861811128784116)\n",
      "('CapitalLoss', ':', -0.1773617505303366)\n",
      "('MaritalStatus_Widowed', ':', 0.1671117388758829)\n",
      "('Occupation_Prof-specialty', ':', 0.16169442039327117)\n",
      "('MaritalStatus_Separated', ':', -0.16136619051419826)\n",
      "('Occupation_Adm-clerical', ':', 0.15749077787279187)\n",
      "('CapitalAvg', ':', -0.14697160470052406)\n",
      "('Occupation_Armed-Forces', ':', 0.1104928136232001)\n",
      "('WorkClass_Federal-gov', ':', -0.10771992718011499)\n",
      "('Occupation_Other-service', ':', -0.08570893153829656)\n"
     ]
    }
   ],
   "source": [
    "feature_names = x_log.columns\n",
    "coef = reg_model_cv_rfe.best_estimator_.coef_[0]\n",
    "\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Feature selection using another model</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 30}\n"
     ]
    }
   ],
   "source": [
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 7), \n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(x_train_log_scaled, y_train_log)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27999, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "selectmodel = SelectFromModel(cv.best_estimator_, prefit=True)\n",
    "x_train_log_scaled_sel = selectmodel.transform(x_train_log_scaled)\n",
    "x_test_log_scaled_sel = selectmodel.transform(x_test_log_scaled)\n",
    "\n",
    "print(x_train_log_scaled_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.8362084360155719)\n",
      "('Test accuracy:', 0.8401666666666666)\n",
      "{'C': 1}\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "reg_model_cv_selection_using_dt = GridSearchCV(\n",
    "    param_grid=params, \n",
    "    estimator=LogisticRegression(random_state=rs), \n",
    "    cv=10, n_jobs=-1)\n",
    "reg_model_cv_selection_using_dt.fit(x_train_log_scaled_sel, y_train_log)\n",
    "\n",
    "print(\"Train accuracy:\", reg_model_cv_selection_using_dt.score(x_train_log_scaled_sel, y_train_log))\n",
    "print(\"Test accuracy:\", reg_model_cv_selection_using_dt.score(x_test_log_scaled_sel, y_test_log))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(reg_model_cv_selection_using_dt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NumWorkingHoursPerWeek', ':', -1.1881493943066028)\n",
      "('CapitalLoss', ':', -0.9807762139353696)\n",
      "('Weighting', ':', -0.8728766612594224)\n",
      "('NumYearsEducation', ':', -0.5940253766158401)\n",
      "('CapitalGain', ':', 0.5793449367701083)\n",
      "('Age', ':', -0.4377330832156579)\n",
      "('CapitalAvg', ':', -0.35342412203272106)\n",
      "('Sex', ':', -0.22395583109399927)\n"
     ]
    }
   ],
   "source": [
    "feature_names = x_log.columns\n",
    "coef = reg_model_cv_selection_using_dt.best_estimator_.coef_[0]\n",
    "\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a. Report which variables are included in the regression model</h5>\n",
    "<p>\n",
    "Using RFE, <br/>\n",
    "Original feature set: 82 (x_train.shape) <br/>\n",
    "Number of features after elimination: 44\n",
    "</p>\n",
    "<p>\n",
    "Using feature selection using another model<br/>\n",
    "Number of features after elimination: 8\n",
    "</p>\n",
    "<h5>b. Report the top-5 important variables in the model</h5>\n",
    "Using RFE,<br/>\n",
    "NumYearsEducation: -0.7084501674400273<br/>\n",
    "WorkClass_State-gov: -0.4605340082385802<br/>\n",
    "Age: -0.36480469670520127<br/>\n",
    "MaritalStatus_Divorced: 0.35113976967417915<br/>\n",
    "NumWorkingHoursPerWeek: -0.3475917918739649<br/><br/>\n",
    "\n",
    "Using feature selection using another model<br/>\n",
    "NumWorkingHoursPerWeek: -1.1881493943066028<br/>\n",
    "CapitalLoss: -0.9807762139353696<br/>\n",
    "Weighting: -0.8728766612594224<br/>\n",
    "NumYearsEducation: -0.5940253766158401<br/>\n",
    "CapitalGain: 0.5793449367701083\n",
    "<h5>a. Report any sign of overfitting</h5>\n",
    "The training accuracy and test accuracy of the model was almost identical. This means there's very little overfitting.\n",
    "<h5>a. What is the classification accuracy on training and test datasets?</h5>\n",
    "<p>\n",
    "Using RFE, <br/>\n",
    "Train accuracy: 0.843315832708311<br/>\n",
    "Test accuracy: 0.8455\n",
    "</p>\n",
    "<p>\n",
    "Using feature selection using another model<br/>\n",
    "Train accuracy: 0.8362084360155719<br/>\n",
    "Test accuracy: 0.8401666666666666\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Using the comparison statistics, which of the regression models appears to be better? Is there any difference between the two models? Explain why those changes may have happened.</h3>\n",
    "<p>\n",
    "There's a very little difference between the two models in terms of the performance. The test accuracy of the model that includes all variables was 0.8445833333333334 and the performance of the model using RFE was 0.8455.\n",
    "</p>\n",
    "<p>\n",
    "Another difference was the feature importance.<br/>\n",
    "The previous model's feature importance was: <br/>\n",
    "NumYearsEducation: -0.7713223988970697<br/>\n",
    "CapitalGain: -0.7092790273413643<br/>\n",
    "MaritalStatus_Married-civ-spouse: -0.6824070765694628<br/>\n",
    "MaritalStatus_Never-married: 0.47490755998987544<br/>\n",
    "CapitalLoss: -0.42201472689663533 <br/><br/>\n",
    "    \n",
    "The model using RFE's feature importance was: <br/>\n",
    "NumYearsEducation: -0.7084501674400273<br/>\n",
    "WorkClass_State-gov: -0.4605340082385802<br/>\n",
    "Age: -0.36480469670520127<br/>\n",
    "MaritalStatus_Divorced: 0.35113976967417915<br/>\n",
    "NumWorkingHoursPerWeek: -0.3475917918739649<br/><br/>\n",
    "\n",
    "Since the model using RFE has reduced the feature set size from 83 to 44, its process is faster than the previous model. Therefore the latter model is better.\n",
    "</p>\n",
    "<h3>From the better model, can you identify which householders to target for providing loan? Can you provide some descriptive summary of those householders?</h3>\n",
    "<p>\n",
    "Based on the feature importance and coefficients the householder: <br/>\n",
    "- Should have high level of education<br/>\n",
    "- Should work at a state government<br/>\n",
    "- Should be aged<br/>\n",
    "- Should not be devorced<br/>\n",
    "- Should have long working hour per week <br/>\n",
    "</p>\n",
    "<h1>Task 4. Predictive Modeling Using Neural Networks</h1>\n",
    "<h3>1. Build a Neural Network model using the default setting</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(200,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=7, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=20, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "('Train accuracy:', 0.7934211936140576)\n",
      "('Test accuracy:', 0.7953333333333333)\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"default\")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nnmodel = MLPClassifier(max_iter=7, random_state=rs)\n",
    "nnmodel.fit(x_train, y_train)\n",
    "print(nnmodel)\n",
    "print(\"Train accuracy:\", nnmodel.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", nnmodel.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a. What are the parameters used? Explain your decision. what is the network architecture?</h5>\n",
    "<p>\n",
    "I have used the maximum iterations to 7 because it provides the better accuracy than using the default maximum iteration of 200. \n",
    "The default parameters used:<br/>\n",
    "- hidden_layer_sizes=(100,): 1 hidden layer with 100 neurons<br/>\n",
    "- alpha=0.0001: L2 regularization parameter used in the activation function<br/>\n",
    "- activation='relu': The activation function used in the neurons is rectified linear unit.<br/>\n",
    "The network has an input layer with 82 features, one hidden layer with 100 neurons and an output layer with a single output neuron.\n",
    "</p>\n",
    "<h5>b.How many iterations are needed to train this network?</h5>\n",
    "7 iterations are needed.\n",
    "<h5>c.Do you see any sign of over-fitting?</h5>\n",
    "The accuracy of the model on training data was less than the accuracy on test dataset therefore there's no over-fitting.\n",
    "<h5>d.Did the training process converge and resulted in the best model?</h5>\n",
    "Maximum iteration parameter below 7 produces a \"convergence is not reached\" warning and maximum iterations more than 7 does not produce better accuracy. Therefore the training process converged and resulted in the best model.\n",
    "<h5>d.Did the training process converge and resulted in the best model?</h5>\n",
    "Train accuracy: 0.7938854959105682<br/>\n",
    "Test accuracy: 0.798<br/>\n",
    "<h3>2. Refine this network by tuning it with GridSearchCV</h3>\n",
    "First tuning with max_iter around value of 7 and hidden_layer_sizes around the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.5698774956248438)\n",
      "('Test accuracy:', 0.5724166666666667)\n",
      "{'max_iter': 6, 'hidden_layer_sizes': (45,)}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_iter': [x for x in range(6, 10, 1)],\n",
    "    'hidden_layer_sizes': [(x,) for x in range(5, 82, 20)]\n",
    "}\n",
    "neural_cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "neural_cv.fit(x_train, y_train)\n",
    "print(\"Train accuracy:\", neural_cv.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", neural_cv.score(x_test, y_test))\n",
    "print(neural_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy: 0.5698774956248438<br/>\n",
    "Test accuracy: 0.5724166666666667<br/>\n",
    "Best Params:<br/>\n",
    "max_iter: 6<br/>\n",
    "hidden_layer_sizes: (45,)<br/>\n",
    "The accuracy is much worse than previous model.<br/><br/>\n",
    "\n",
    "Adjusted hidden_layer_sizes to range between 100 to 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.7943497982070788)\n",
      "('Test accuracy:', 0.7955833333333333)\n",
      "{'max_iter': 9, 'hidden_layer_sizes': (160,)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_iter': [x for x in range(6, 10, 1)],\n",
    "    'hidden_layer_sizes': [(x,) for x in range(100, 200, 20)]\n",
    "}\n",
    "\n",
    "neural_cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "neural_cv.fit(x_train, y_train)\n",
    "print(\"Train accuracy:\", neural_cv.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", neural_cv.score(x_test, y_test))\n",
    "print(neural_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy has improved to:<br/>\n",
    "Train accuracy: 0.7943497982070788<br/>\n",
    "Test accuracy: 0.7955833333333333<br/>\n",
    "Best Params:<br/>\n",
    "max_iter: 9<br/>\n",
    "hidden_layer_sizes: (160,)<br/>\n",
    "The model can be tuned further based on this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.783492267580985)\n",
      "('Test accuracy:', 0.7879166666666667)\n",
      "{'alpha': 0.001, 'max_iter': 11, 'hidden_layer_sizes': (160,)}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_iter': [9, 10, 11],\n",
    "    'hidden_layer_sizes': [(160,),(165,), (170,)],\n",
    "    'alpha': [0.001, 0.0001, 0.00001]\n",
    "}\n",
    "neural_cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "neural_cv.fit(x_train, y_train)\n",
    "print(\"Train accuracy:\", neural_cv.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", neural_cv.score(x_test, y_test))\n",
    "print(neural_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a. What are the parameters used? Explain your decision. What is the network architecture?</h5>\n",
    "The parameter used were:<br/>\n",
    "- max_iter: The range of the max iterations was [9, 10, 11] which is based around 7 because the previous model trained using this value provided better accuracy<br/>\n",
    "- hidden_layer_sizes: The range of values used at first was 5 to 82. This was based on the size of the feature set which is 82. This resulted in poor accuracy so the values were changed to 160, 165 and 170. This produced far better accuracy.<br/>\n",
    "- alpha: The range of this parameter was [0.001, 0.0001, 0.00001] based around the default value of 0.0001<br/>\n",
    "\n",
    "The model had 160 neurons in the hidden layer and the max iteration used was 11 and alpha was 0.001.\n",
    "\n",
    "<h5>b. How many iterations are needed to train this network?</h5>\n",
    "10 iterations were required.\n",
    "\n",
    "<h5>c. Do you see any sign of over-fitting?</h5>\n",
    "The accuracy on the training dataset was 0.7957 and accuracy on the test dataset was 0.7976. Since the accuracy on test dataset is better, there's no over-fitting.\n",
    "<h5>d. Did the training process converge and resulted in the best model?</h5>\n",
    "The neural network achieved convergence before the max iterations. \n",
    "\n",
    "<h5>e. What is classification accuracy on training and test datasets?</h5>\n",
    "Train accuracy: 0.783492267580985<br/>\n",
    "Test accuracy: 0.7879166666666667<br/>\n",
    "\n",
    "<h3>3. Which feature selection help here? Build another Neural Network model with inputs selected from RFE with regression and from decision tree</h3>\n",
    "Using RFE:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.8553162612950462)\n",
      "('Test accuracy:', 0.8489166666666667)\n",
      "{'alpha': 0.001, 'max_iter': 11, 'hidden_layer_sizes': (165,)}\n"
     ]
    }
   ],
   "source": [
    "x_train_log_scaled_sel = rfe.transform(x_train_log_scaled)\n",
    "x_test_log_scaled_sel = rfe.transform(x_test_log_scaled)\n",
    "params = {\n",
    "    'max_iter': [9, 10, 11],\n",
    "    'hidden_layer_sizes': [(160,),(165,), (170,)],\n",
    "    'alpha': [0.001, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "neural_cv_rfe = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "neural_cv_rfe.fit(x_train_log_scaled_sel, y_train_log)\n",
    "print(\"Train accuracy:\", neural_cv_rfe.score(x_train_log_scaled_sel, y_train_log))\n",
    "print(\"Test accuracy:\", neural_cv_rfe.score(x_test_log_scaled_sel, y_test_log))\n",
    "print(neural_cv_rfe.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting using decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 20}\n",
      "(27999, 7)\n"
     ]
    }
   ],
   "source": [
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 7), \n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(x_train_log_scaled, y_train_log)\n",
    "print(cv.best_params_)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "selectmodel = SelectFromModel(cv.best_estimator_, prefit=True)\n",
    "x_train_log_scaled_sel = selectmodel.transform(x_train_log_scaled)\n",
    "x_test_log_scaled_sel = selectmodel.transform(x_test_log_scaled)\n",
    "\n",
    "print(x_train_log_scaled_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.843315832708311)\n",
      "('Test accuracy:', 0.8419166666666666)\n",
      "{'alpha': 0.0001, 'max_iter': 9, 'hidden_layer_sizes': (160,)}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_iter': [9, 10, 11],\n",
    "    'hidden_layer_sizes': [(160,),(165,), (170,)],\n",
    "    'alpha': [0.001, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "neural_cv_decision_tree = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "neural_cv_decision_tree.fit(x_train_log_scaled_sel, y_train_log)\n",
    "print(\"Train accuracy:\", neural_cv_decision_tree.score(x_train_log_scaled_sel, y_train_log))\n",
    "print(\"Test accuracy:\", neural_cv_decision_tree.score(x_test_log_scaled_sel, y_test_log))\n",
    "print(neural_cv_decision_tree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a. Did feature selection help here? Which method of feature selection produced the best result? Any change in the network architecture? What inputs are being used as the network input?</h5>\n",
    "Both feature selection improved the accuracy of the model.\n",
    "The test accuracy was higher in using input selected from RFE. The number of neurons in the hidden layer increased from 160 to 165. \n",
    "\n",
    "<h5>b. What is classification accuracy on training and test datasets? Is there any improvement in the outcome?</h5>\n",
    "Selection using RFE produced the following result: <br/>\n",
    "Train accuracy: 0.8553162612950462<br/>\n",
    "Test accuracy: 0.8489166666666667<br/><br/>\n",
    "\n",
    "Selection using decision tree produced the following result:<br/>\n",
    "Train accuracy: 0.843315832708311<br/>\n",
    "Test accuracy: 0.8419166666666666<br/><br/>\n",
    "\n",
    "The test accuracy when using RFE improved by 0.061.\n",
    "<h5>c. How many iterations are now needed to train this network?</h5>\n",
    "Maximum of 9 iterations\n",
    "<h5>d. Did you see any sign of over-fitting?</h5>\n",
    "The accuracy on the training dataset was only slightly higher than test dataset. Therefore there's no sign of over-fitting.\n",
    "\n",
    "<h5>e. Did the training process converge and resulted in the best model?</h5>\n",
    "\n",
    "Yes.\n",
    "\n",
    "\n",
    "<h5>f.</h5>\n",
    "The best parameter produced when using RFE was \n",
    "{'alpha': 0.0001, 'max_iter': 9, 'hidden_layer_sizes': (160,)}\n",
    "\n",
    "Based on the result, max_iteration will be tuned around 9 and hidden_layer_sizes around 160."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy:', 0.843315832708311)\n",
      "('Test accuracy:', 0.8419166666666666)\n",
      "{'alpha': 0.0001, 'max_iter': 9, 'hidden_layer_sizes': (160,)}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_iter': [8, 9, 10],\n",
    "    'hidden_layer_sizes': [(155,),(160,), (165,)],\n",
    "    'alpha': [0.001, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "neural_cv_rfe = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "neural_cv_rfe.fit(x_train_log_scaled_sel, y_train_log)\n",
    "print(\"Train accuracy:\", neural_cv_rfe.score(x_train_log_scaled_sel, y_train_log))\n",
    "print(\"Test accuracy:\", neural_cv_rfe.score(x_test_log_scaled_sel, y_test_log))\n",
    "print(neural_cv_rfe.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the parameters did not improve the accuracy.\n",
    "\n",
    "<h1>Task 5. Comparing Predictive Models</h1>'\n",
    "<h3>1. Use the comparison methods to compare the best decision tree model, the best regression model, and the best neural network model.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy score on test for DT:', 0.8555833333333334)\n",
      "('Accuracy score on test for logistic regression:', 0.8455)\n",
      "('Accuracy score on test for NN:', 0.8489166666666667)\n"
     ]
    }
   ],
   "source": [
    "y_pred_dt = dt_model_cv_best.predict(x_test)\n",
    "y_pred_log_reg = reg_model_cv_rfe.predict(x_test_log_scaled_sel)\n",
    "y_pred_nn = neural_cv_rfe.predict(x_test_log_scaled_sel)\n",
    "print(\"Accuracy score on test for DT:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Accuracy score on test for logistic regression:\", accuracy_score(y_test_log, y_pred_log_reg))\n",
    "print(\"Accuracy score on test for NN:\", accuracy_score(y_test_log, y_pred_nn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on test accuracy score, the decision tree performs the best, followed by neural network and logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROC index on test for DT:', 0.8957314543188573)\n",
      "('ROC index on test for logistic regression:', 0.8953244751222319)\n",
      "('ROC index on test for NN:', 0.9023080049851404)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_proba_dt = dt_model_cv_best.predict_proba(x_test)\n",
    "y_pred_proba_log_reg = reg_model_cv_rfe.predict_proba(x_test_log_scaled_sel)\n",
    "y_pred_proba_nn = neural_cv_rfe.predict_proba(x_test_log_scaled_sel)\n",
    "\n",
    "roc_index_dt = roc_auc_score(y_test, y_pred_proba_dt[:, 1])\n",
    "roc_index_log_reg = roc_auc_score(y_test_log, y_pred_proba_log_reg[:, 1])\n",
    "roc_index_nn = roc_auc_score(y_test_log, y_pred_proba_nn[:, 1])\n",
    "\n",
    "print(\"ROC index on test for DT:\", roc_index_dt)\n",
    "print(\"ROC index on test for logistic regression:\", roc_index_log_reg)\n",
    "print(\"ROC index on test for NN:\", roc_index_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network model has the best ROC score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_pred_proba_dt[:,1])\n",
    "fpr_log_reg, tpr_log_reg, thresholds_log_reg = roc_curve(y_test_log, y_pred_proba_log_reg[:,1])\n",
    "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_test_log, y_pred_proba_nn[:,1])\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr_dt, tpr_dt, label='ROC Curve for DT {:.3f}'.format(roc_index_dt), color='red', lw=0.5)\n",
    "plt.plot(fpr_log_reg, tpr_log_reg, label='ROC Curve for Log reg {:.3f}'.format(roc_index_log_reg), color='green', lw=0.5)\n",
    "plt.plot(fpr_nn, tpr_nn, label='ROC Curve for NN {:.3f}'.format(roc_index_nn), color='darkorange', lw=0.5)\n",
    "\n",
    "# plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "#          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5>a. Discuss the findings led by:</h5>\n",
    "i. ROC Chart and Index<br/>\n",
    "The ROC score is as follows:<br/>\n",
    "ROC index on test for DT: 0.8957314543188573<br/>\n",
    "ROC index on test for logistic regression: 0.8953244751222319<br/>\n",
    "ROC index on test for NN: 0.9023080049851404<br/>\n",
    "Neural network model has highest ROC score<br/>\n",
    "Also the plot shows that the neural network model is cloest to the top left corner and has largest area under the curve.<br/>\n",
    "\n",
    "ii. Accuracy Score<br/>\n",
    "The accuracy score is as follows:<br/>\n",
    "Accuracy score on test for DT: 0.8555833333333334<br/>\n",
    "Accuracy score on test for logistic regression: 0.8455<br/>\n",
    "Accuracy score on test for NN: 0.8489166666666667<br/>\n",
    "Based on test accuracy score, the decision tree performs the best, followed by neural network and logistic regression. <br/>\n",
    "\n",
    "<h5>b. Which model would you use in deployment based on these findings?</h5>\n",
    "The accuracy scores, ROC score and AUC all vary slightly.<br/>\n",
    "This means performance of these models are similar and comparison should be made using other criteria mainly interpretability, speed and adaptability.<br/><br/>\n",
    "\n",
    "Decision tree is easy to interpret and fast to train and predict however neural network is better at adapting in a changing environment.<br/><br/>\n",
    "\n",
    "Therefore I would choose decision tree because it has slightly higher accuracy score than other models and it is easy to interpret and has fast processing speed.<br/>\n",
    "\n",
    "<h5>c. Do all the models agree on the householder's characteristics?</h5>\n",
    "Yes to some extent. The models agree that the householder should:<br/>\n",
    "- Be married<br/>\n",
    "- Should have a high level of education<br/>\n",
    "\n",
    "<h3>2. How can the outcome of this study be used by decision makers?</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
