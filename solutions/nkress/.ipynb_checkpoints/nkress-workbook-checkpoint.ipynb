{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import dataset\n",
    "df = pd.read_csv('./HouseholderAtRisk.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Data selection and distribution (4 marks)\n",
    "\n",
    "1. What is the proportion of householders who have high risk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High    30497\n",
      "Low      9501\n",
      "Name: High, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['High'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a:\n",
    "76.246% of householders have high risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Did you have to fix any data quality problems? Detail them.\n",
    "    Apply the imputation method(s) to the variable(s) that need it. List the variables that needed it. Justify your choise of imputation if needed\n",
    "\n",
    "Applied correct names to attribute columns\n",
    "\n",
    "//TODO\n",
    "Some occupations numeric values\n",
    "mising values for entries\n",
    "tuples empty except one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39998 entries, 0 to 39997\n",
      "Data columns (total 18 columns):\n",
      "id                            39998 non-null int64\n",
      "age                           39031 non-null float64\n",
      "work_class                    39026 non-null object\n",
      "weighting                     38706 non-null float64\n",
      "education                     39026 non-null object\n",
      "num_years_education           39026 non-null float64\n",
      "marital_status                39026 non-null object\n",
      "occupation                    39012 non-null object\n",
      "relationship                  39026 non-null object\n",
      "race                          45 non-null object\n",
      "gender                        39026 non-null object\n",
      "capital_loss                  39026 non-null float64\n",
      "capital_gain                  39026 non-null float64\n",
      "capital_avg                   39026 non-null float64\n",
      "num_working_hours_per_week    39026 non-null float64\n",
      "sex                           39026 non-null float64\n",
      "country_of_origin             39968 non-null object\n",
      "at_risk                       39998 non-null object\n",
      "dtypes: float64(8), int64(1), object(9)\n",
      "memory usage: 5.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# list unique values for each column\n",
    "# check data matches description\n",
    "\n",
    "# for column in df.columns:\n",
    "#     print('\\nColumn name: ' + column )\n",
    "#     print(df[column].unique())\n",
    "    \n",
    "# rename each column to correct attribute name \n",
    "# according to task description\n",
    "df.rename(columns= {\n",
    "    '1': 'id',\n",
    "    '25': 'age',\n",
    "    ' Private': 'work_class',\n",
    "    '224942': 'weighting',\n",
    "    ' 11th': 'education',\n",
    "    '7': 'num_years_education',\n",
    "    ' Never-married': 'marital_status',\n",
    "    ' Machine-op-inspct': 'occupation',\n",
    "    ' Own-child': 'relationship',\n",
    "    'Unnamed: 9': 'race',\n",
    "    ' Male': 'gender',\n",
    "    '0': 'capital_loss',\n",
    "    '0.1': 'capital_gain',\n",
    "    '0.2': 'capital_avg',\n",
    "    '40': 'num_working_hours_per_week',\n",
    "    '0.3': 'sex',\n",
    "    ' US': 'country_of_origin',\n",
    "    'High': 'at_risk',\n",
    "    \n",
    "}, inplace=True)\n",
    "\n",
    "#print new column names\n",
    "print('\\n')\n",
    "print(df.info())\n",
    "\n",
    "## correct column types\n",
    "\n",
    "\n",
    "\n",
    "## Average missing data points\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove near empty columns\n",
    "df = df.drop(columns=['race', 'capital_loss', 'capital_gain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove rows with nothing but ID\n",
    "df.dropna(thresh=8, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove rows in occupation with ? value as hard to imputate media\n",
    "df = df[df.occupation != '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove rows in weighting that are na as hard to imputate median\n",
    "df.dropna(subset=['weighting'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imputate rows with ? in workClass and Country of Origin as median is easy to determine\n",
    "df['work_class'].replace('?', df['work_class'].mode(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputate rows ? in CountryOfOrigin to median of origin\n",
    "df['country_of_origin'].replace('?', df['country_of_origin'].mode(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change sex to boolean map\n",
    "df['sex'] = df['sex'].map({'M': 0, 'F':1 })\n",
    "# change from float to int\n",
    "df['age'] = df['age'].astype(int);\n",
    "df['num_years_education'] = df['num_years_education'].astype(int)\n",
    "df['num_working_hours_per_week'] = df['num_working_hours_per_week'].astype(int)\n",
    "df['weighting'] = df['weighting'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The dataset may include irrelevant and redundant variables. What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "\n",
    "\n",
    "* Gender - sex is already provided\n",
    "* Race - almost no entries in column 45/39998\n",
    "* capital columns - majority of data == 0\n",
    "* CountryOfOrigin - majority of data from USA\n",
    "* Weighting - irrelevant to objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39998 entries, 0 to 39997\n",
      "Data columns (total 11 columns):\n",
      "id                            39998 non-null int64\n",
      "age                           39031 non-null float64\n",
      "work_class                    39026 non-null object\n",
      "education                     39026 non-null object\n",
      "num_years_education           39026 non-null float64\n",
      "marital_status                39026 non-null object\n",
      "occupation                    39012 non-null object\n",
      "relationship                  39026 non-null object\n",
      "num_working_hours_per_week    39026 non-null float64\n",
      "sex                           0 non-null float64\n",
      "at_risk                       39998 non-null object\n",
      "dtypes: float64(4), int64(1), object(6)\n",
      "memory usage: 4.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print(df['Race'].value_counts(dropna=False), '\\n')\n",
    "\n",
    "# print(df['CapitalLoss'].value_counts(bins = 5), '\\n')\n",
    "# print(df['CapitalGain'].value_counts(bins = 5), '\\n')\n",
    "# print(df['CapitalAvg'].value_counts(bins=5), '\\n')\n",
    "\n",
    "# print(df['CountryOfOrigin'].value_counts(), '\\n')\n",
    "\n",
    "## Drop redundant columns\n",
    "df = df.drop(columns=['gender', 'capital_avg', 'weighting', 'country_of_origin'])\n",
    "## employment type ?? as majority is private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What distribution scheme did you use? What “data partitioning allocation” did you set? Explain your selection. (Hint: Take the lead from Week 2 lecture on data distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Predictive Modelling Using Decision Trees (4 marks)\n",
    "1. Build a decision tree using the default setting. Examine the tree results and answer the followings:\n",
    "\n",
    "  a. What is classification accuracy on training and test datasets?\n",
    "  \n",
    "  b. Which variable is used for the first split? What are the variables that are used for the second split?\n",
    "  \n",
    "  c. What are the 5 important variables in building the tree?\n",
    "  \n",
    "  d. Report if you see any evidence of model overfitting.\n",
    "  \n",
    "  \n",
    "2. Build another decision tree tuned with GridSearchCV. Examine the tree results.\n",
    "  \n",
    "  a. What is classification accuracy on training and test datasets?\n",
    "  \n",
    "  b. What are the parameters used? Explain your decision.\n",
    "  \n",
    "  c. What are the optimal parameters for this decision tree?\n",
    "  \n",
    "  d. Which variable is used for the first split? What are the variables that are used for the second split?\n",
    "  \n",
    "  e. What are the 5 important variables in building the tree?\n",
    "  \n",
    "  f. Report if you see any evidence of model overfitting.\n",
    "  \n",
    "  \n",
    "3. What is the significant difference do you see between these two decision tree models – default (Task 2.1) and using GridSearchCV (Task 2.2)? How do theycompare performance-wise? Explain why those changes may have happened.\n",
    "\n",
    "\n",
    "4. From the better model, can you identify which householders to target for providing loan? Can you provide some descriptive summary of those householders?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "## Predictive Modeling Using Regression (5.5 marks)\n",
    "1. Describe why you will have to do additional preparation for variables to be\n",
    "used in regression modelling. Apply transformation method(s) to the\n",
    "variable(s) that need it. List the variables that needed it.\n",
    "2. Build a regression model using the default regression method with all\n",
    "inputs. Once you have completed it, build another model and tune it usingGridSearchCV. Answer the followings:\n",
    "a. Report which variables are included in the regression model.\n",
    "b. Report the top-5 important variables (in the order) in the model.\n",
    "c. Report any sign of overfitting.\n",
    "d. What are the parameters used? Explain your decision. What are the\n",
    "optimal parameters? Which regression function is being used?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "3. Build another regression model using the subset of inputs selected either\n",
    "by RFE or the selection by model method. Answer the followings:\n",
    "a. Report which variables are included in the regression model.\n",
    "b. Report the top-5 important variables (in the order) in the model.\n",
    "c. Report any sign of overfitting.\n",
    "d. What is classification accuracy on training and test datasets?\n",
    "4. Using the comparison statistics, which of the regression models appears to\n",
    "be better? Is there any difference between the two models (i.e one with\n",
    "selected variables and another with all variables)? Explain why those\n",
    "changes may have happened.\n",
    "5. From the better model, can you identify which householders to target for\n",
    "providing loan? Can you provide some descriptive summary of those\n",
    "householders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## Predictive Modeling Using Neural Networks (5.5 marks)\n",
    "1. Build a Neural Network model using the default setting. Answer the\n",
    "following:\n",
    "a. What are the parameters used? Explain your decision. What is the\n",
    "network architecture?\n",
    "b. How many iterations are needed to train this network?\n",
    "c. Do you see any sign of over-fitting?\n",
    "d. Did the training process converge and resulted in the best model?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "2. Refine this network by tuning it with GridSearchCV. Answer the\n",
    "following:\n",
    "a. What are the parameters used? Explain your decision. What is the\n",
    "network architecture?\n",
    "b. How many iterations are needed to train this network?\n",
    "c. Do you see any sign of over-fitting?\n",
    "d. Did the training process converge and resulted in the best model?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "3. Would feature selection help here? Build another Neural Network model\n",
    "with inputs selected from RFE with regression (use the best model\n",
    "generated in Task 3) and from the decision tree (use the best model\n",
    "from Task 2). Answer the following for the best neural network model:a. Did feature selection help here? Which method of feature selection\n",
    "produced the best result? Any change in the network architecture?\n",
    "What inputs are being used as the network input?\n",
    "b. What is classification accuracy on training and test datasets? Is there\n",
    "any improvement in the outcome?\n",
    "c. How many iterations are now needed to train this network?\n",
    "d. Do you see any sign of over-fitting?\n",
    "e. Did the training process converge and resulted in the best model?\n",
    "f. Finally, see whether the change in network architecture can further\n",
    "improve the performance, use GridSearchCV to tune the network.\n",
    "Report if there was any improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "## Comparing Predictive Models (4 marks)\n",
    "1. Use the comparison methods to compare the best decision tree model, the\n",
    "best regression model, and the best neural network model.\n",
    "a. Discuss the findings led by:\n",
    "(i) ROC Chart and Index;\n",
    "(ii) Accuracy Score;\n",
    "b. Which model would you use in deployment based on these findings?\n",
    "Discuss why?\n",
    "c. Do all the models agree on the householder’s characteristics? How do\n",
    "they vary?\n",
    "2. How the outcome of this study can be used by decision makers?\n",
    "3. Can you summarise the positives and negative aspects of each predictive\n",
    "modelling method based on this data analysis exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
