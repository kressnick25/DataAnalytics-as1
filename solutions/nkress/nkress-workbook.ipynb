{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import dataset\n",
    "df = pd.read_csv('./HouseholderAtRisk.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Data selection and distribution (4 marks)\n",
    "\n",
    "1. What is the proportion of householders who have high risk?\n",
    "\n",
    "**76.246% of householders have high risk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       High\n",
      "High  30497\n",
      "Low    9501\n",
      "\n",
      " High percentage: 76.24631231561578 %\n"
     ]
    }
   ],
   "source": [
    "print(df['High'].value_counts().to_frame())\n",
    "\n",
    "total = 30497 + 9501\n",
    "print('\\n High percentage:', (30497 / total) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Did you have to fix any data quality problems? Detail them.\n",
    "    Apply the imputation method(s) to the variable(s) that need it. List the variables that needed it. Justify your choise of imputation if needed\n",
    "\n",
    "**Data quality problems**\n",
    "1. Renamed each column with corresponding name\n",
    "2. Drop rows with over 95% NaN values\n",
    "3. Drop rows with more than 8 empty cells\n",
    "\n",
    "\n",
    "**Imputation**\n",
    "1. imputate `occupation` by removal, small percentage to remove and no simple median to apply\n",
    "2. imputate rows in `weighting` by removal, small percentage to remove and no simple median to apply\n",
    "3. imputate rows in `work_class` by applying median value. median value is large majority\n",
    "4. imputate rows in `country_of_origin` by applying median value. median value is large majority\n",
    "\n",
    "**DataTypes**\n",
    "1. Change all object types to categorical, then convert catergorical to representitive `int` value\n",
    "\n",
    "**Bin Values**\n",
    "# TODO\n",
    "1. Bin `weighting` to remove problem of large range of singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list unique values for each column\n",
    "# check data matches description\n",
    "\n",
    "# for column in df.columns:\n",
    "#     print('\\nColumn name: ' + column )\n",
    "#     print(df[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA QUALITY\n",
    "## 1\n",
    "# rename each column to correct attribute name \n",
    "# according to task description\n",
    "df.rename(columns= {\n",
    "    '1': 'id',\n",
    "    '25': 'age',\n",
    "    ' Private': 'work_class',\n",
    "    '224942': 'weighting',\n",
    "    ' 11th': 'education',\n",
    "    '7': 'num_years_education',\n",
    "    ' Never-married': 'marital_status',\n",
    "    ' Machine-op-inspct': 'occupation',\n",
    "    ' Own-child': 'relationship',\n",
    "    'Unnamed: 9': 'race',\n",
    "    ' Male': 'gender',\n",
    "    '0': 'capital_loss',\n",
    "    '0.1': 'capital_gain',\n",
    "    '0.2': 'capital_avg',\n",
    "    '40': 'num_working_hours_per_week',\n",
    "    '0.3': 'sex',\n",
    "    ' US': 'country_of_origin',\n",
    "    'High': 'at_risk',\n",
    "    \n",
    "}, inplace=True)\n",
    "\n",
    "## 2\n",
    "# Remove near empty columns\n",
    "df.drop(columns=['race', 'capital_loss', 'capital_gain', 'capital_avg'], inplace=True)\n",
    "\n",
    "## 3\n",
    "# Remove rows with nothing but ID\n",
    "df.dropna(thresh=8, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPUTATION\n",
    "## 1\n",
    "# remove rows in 'occupation' with ? value as hard to imputate media\n",
    "### TODO WHY WONT THIS LINE WORK\n",
    "df.drop(df.loc[df['occupation']== '?'].index, inplace=True)\n",
    "\n",
    "## 2\n",
    "# remove rows in 'weighting' that are NaN as hard to imputate median\n",
    "df.dropna(subset=['weighting'], inplace=True)\n",
    "\n",
    "## 3\n",
    "# imputate rows with ? in 'work_class' as median is easy to determine\n",
    "df['work_class'].replace('?', df['work_class'].mode(), inplace=True)\n",
    "\n",
    "## 4\n",
    "# imputate rows ? in country_of_origin to median of origin\n",
    "df['country_of_origin'].replace('?', df['country_of_origin'].mode(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prof-specialty       4902\n",
      " Craft-repair         4811\n",
      " Exec-managerial      4775\n",
      " Adm-clerical         4508\n",
      " Sales                4365\n",
      " Other-service        3905\n",
      " Machine-op-inspct    2412\n",
      " ?                    2230\n",
      " Transport-moving     1854\n",
      " Handlers-cleaners    1605\n",
      " Farming-fishing      1185\n",
      " Tech-support         1144\n",
      " Protective-serv       777\n",
      " Priv-house-serv       208\n",
      " Armed-Forces           11\n",
      "Name: occupation, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['occupation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### DATA TYPES\n",
    "# ## 1\n",
    "# # change objects to category\n",
    "# df['marital_status'] = df['marital_status'].astype('category')\n",
    "# df['occupation'] = df['occupation'].astype('category')\n",
    "# df['relationship'] = df['relationship'].astype('category')\n",
    "# df['work_class'] = df['work_class'].astype('category')\n",
    "\n",
    "\n",
    "# ### TODO USE CATEGORIES IN MODEL NOT INT\n",
    "# cat_columns = df.select_dtypes(['category']).columns\n",
    "# df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "# print (df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2\n",
    "# change from float to int\n",
    "df['age'] = df['age'].astype(int);\n",
    "df['num_years_education'] = df['num_years_education'].astype(int)\n",
    "df['num_working_hours_per_week'] = df['num_working_hours_per_week'].astype(int)\n",
    "df['weighting'] = df['weighting'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary var\n",
    "dem_home_owner_map = {'High':0, 'Low': 1}\n",
    "df['at_risk'] = df['at_risk'].map(dem_home_owner_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 38706 entries, 0 to 39997\n",
      "Data columns (total 14 columns):\n",
      "id                            38706 non-null int64\n",
      "age                           38706 non-null int64\n",
      "work_class                    38706 non-null object\n",
      "weighting                     38706 non-null int64\n",
      "education                     38706 non-null object\n",
      "num_years_education           38706 non-null int64\n",
      "marital_status                38706 non-null object\n",
      "occupation                    38692 non-null object\n",
      "relationship                  38706 non-null object\n",
      "gender                        38706 non-null object\n",
      "num_working_hours_per_week    38706 non-null int64\n",
      "sex                           38706 non-null float64\n",
      "country_of_origin             38706 non-null object\n",
      "at_risk                       38706 non-null int64\n",
      "dtypes: float64(1), int64(6), object(7)\n",
      "memory usage: 5.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "### Bin rows\n",
    "##df['weighting'] = pd.cut(df['weighting'], 20)\n",
    "##print(df['weighting'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convert target variable to be binary value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The dataset may include irrelevant and redundant variables. What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "\n",
    "**1. Large majority of cells are single value**\n",
    "`country_of_origin`, `work_class`\n",
    "\n",
    "**2. Other**\n",
    "\n",
    "`gender`: `sex` column describes same data\n",
    "`education`: `num_years_education` better descriptor\n",
    "`id`: not data just identifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['Race'].value_counts(dropna=False), '\\n')\n",
    "# print(df['CapitalLoss'].value_counts(bins = 5), '\\n')\n",
    "# print(df['CapitalGain'].value_counts(bins = 5), '\\n')\n",
    "# print(df['CapitalAvg'].value_counts(bins=5), '\\n')\n",
    "# print(df['CountryOfOrigin'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['country_of_origin' 'work_class'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-effdf351f912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country_of_origin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'work_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m## 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'education'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['country_of_origin' 'work_class'] not found in axis\""
     ]
    }
   ],
   "source": [
    "## 1\n",
    "df.drop(columns=['country_of_origin', 'work_class'], inplace=True)\n",
    "## 2\n",
    "df.drop(columns=['gender', 'education', 'id'], inplace=True)\n",
    "\n",
    "# perform one hot encoding minus target variable\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What distribution scheme did you use? What “data partitioning allocation” did you set? Explain your selection. (Hint: Take the lead from Week 2 lecture on data distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to y = target value, x  = rest\n",
    "y = df['at_risk']\n",
    "X = df.drop(['at_risk'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Predictive Modelling Using Decision Trees (4 marks)\n",
    "1. Build a decision tree using the default setting. Examine the tree results and answer the followings:\n",
    "\n",
    "  a. What is classification accuracy on training and test datasets?\n",
    "  \n",
    "     ___Train accuracy:___  0.9998154572968185\n",
    "     \n",
    "     ___Test accuracy:___  0.7732518084739924\n",
    "  \n",
    "  b. Which variable is used for the first split? What are the variables that are used for the second split?\n",
    "  \n",
    "  c. What are the 5 important variables in building the tree?\n",
    "  \n",
    "  `weighting : 0.27891168563163`\n",
    "  \n",
    "`marital_status_ Married-civ-spouse : 0.2001390885510994`\n",
    "\n",
    "`age : 0.14475803491163708`\n",
    "\n",
    "`num_years_education : 0.1386242604759175`\n",
    "\n",
    "`num_working_hours_per_week : 0.09825072789925877`\n",
    "\n",
    "  \n",
    "  d. Report if you see any evidence of model overfitting.\n",
    "  There is evidence of overfitting in that there is a large difference in training accuracy compared to test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 38706 entries, 0 to 39997\n",
      "Data columns (total 33 columns):\n",
      "age                                      38706 non-null int64\n",
      "weighting                                38706 non-null int64\n",
      "num_years_education                      38706 non-null int64\n",
      "num_working_hours_per_week               38706 non-null int64\n",
      "sex                                      38706 non-null float64\n",
      "marital_status_ Divorced                 38706 non-null uint8\n",
      "marital_status_ Married-AF-spouse        38706 non-null uint8\n",
      "marital_status_ Married-civ-spouse       38706 non-null uint8\n",
      "marital_status_ Married-spouse-absent    38706 non-null uint8\n",
      "marital_status_ Never-married            38706 non-null uint8\n",
      "marital_status_ Separated                38706 non-null uint8\n",
      "marital_status_ Widowed                  38706 non-null uint8\n",
      "occupation_ ?                            38706 non-null uint8\n",
      "occupation_ Adm-clerical                 38706 non-null uint8\n",
      "occupation_ Armed-Forces                 38706 non-null uint8\n",
      "occupation_ Craft-repair                 38706 non-null uint8\n",
      "occupation_ Exec-managerial              38706 non-null uint8\n",
      "occupation_ Farming-fishing              38706 non-null uint8\n",
      "occupation_ Handlers-cleaners            38706 non-null uint8\n",
      "occupation_ Machine-op-inspct            38706 non-null uint8\n",
      "occupation_ Other-service                38706 non-null uint8\n",
      "occupation_ Priv-house-serv              38706 non-null uint8\n",
      "occupation_ Prof-specialty               38706 non-null uint8\n",
      "occupation_ Protective-serv              38706 non-null uint8\n",
      "occupation_ Sales                        38706 non-null uint8\n",
      "occupation_ Tech-support                 38706 non-null uint8\n",
      "occupation_ Transport-moving             38706 non-null uint8\n",
      "relationship_ Husband                    38706 non-null uint8\n",
      "relationship_ Not-in-family              38706 non-null uint8\n",
      "relationship_ Other-relative             38706 non-null uint8\n",
      "relationship_ Own-child                  38706 non-null uint8\n",
      "relationship_ Unmarried                  38706 non-null uint8\n",
      "relationship_ Wife                       38706 non-null uint8\n",
      "dtypes: float64(1), int64(4), uint8(28)\n",
      "memory usage: 4.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X.info())\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# setting random state\n",
    "rs = 10\n",
    "\n",
    "# To ignore any future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time:  0.1648106575012207\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "# record how long processing takes to execute\n",
    "import time \n",
    "startTime = time.time()\n",
    "\n",
    "# simple decision tree training\n",
    "model = DecisionTreeClassifier(random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('processing time: ', time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9998154572968185\n",
      "Test accuracy:  0.7710127454357562\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy: ', model.score(X_train, y_train))\n",
    "print('Test accuracy: ', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighting : 0.27891168563163\n",
      "marital_status_ Married-civ-spouse : 0.2001390885510994\n",
      "age : 0.14475803491163708\n",
      "num_years_education : 0.1386242604759175\n",
      "num_working_hours_per_week : 0.09825072789925877\n",
      "occupation_ Exec-managerial : 0.01583668211946095\n",
      "occupation_ Prof-specialty : 0.011427652472781123\n",
      "occupation_ Sales : 0.011095113485341845\n",
      "sex : 0.010163935951197902\n",
      "occupation_ Craft-repair : 0.009974219715049467\n",
      "occupation_ Adm-clerical : 0.008357981971552821\n",
      "occupation_ Transport-moving : 0.008102079169967963\n",
      "occupation_ Protective-serv : 0.007032222578005483\n",
      "occupation_ Other-service : 0.006679031552365608\n",
      "occupation_ Machine-op-inspct : 0.005226577592261843\n",
      "occupation_ Farming-fishing : 0.005015773148408321\n",
      "occupation_ Tech-support : 0.004437663751866488\n",
      "occupation_ Handlers-cleaners : 0.004413447913397351\n",
      "relationship_ Husband : 0.003978063181628557\n",
      "marital_status_ Divorced : 0.003944933022317757\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=20):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "\n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    for i in indices:\n",
    "       print(feature_names[i], ':', importances[i])\n",
    "\n",
    "def visualize_decision_tree(dm_model, feature_names, save_name):\n",
    "    import pydot\n",
    "    from io import StringIO\n",
    "    from sklearn.tree import export_graphviz\n",
    "    \n",
    "    dotfile = StringIO()\n",
    "    export_graphviz(dm_model, out_file=dotfile, feature_names=feature_names)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png(save_name) # saved in the following file\n",
    "\n",
    "analyse_feature_importance(model, X.columns, 20)\n",
    "visualize_decision_tree(model, X.columns, \"2.1tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build another decision tree tuned with GridSearchCV. Examine the tree results.\n",
    "  \n",
    "  a. What is classification accuracy on training and test datasets?\n",
    "  \n",
    "      `Train accuracy: 0.8306267070200044`\n",
    "      \n",
    "      `Test accuracy: 0.8292283844299001`\n",
    "  \n",
    "  b. What are the parameters used? Explain your decision.\n",
    "  \n",
    "  c. What are the optimal parameters for this decision tree?\n",
    "  \n",
    "  d. Which variable is used for the first split? What are the variables that are used for the second split?\n",
    "  ___First Split:___ `marital_status_ Marries-civ-spouse`\n",
    "  ___Second Split:___ \n",
    "      `num_years_education <= 12.5 true` \n",
    "      `num_years_education <=11.5 false`\n",
    "  e. What are the 5 important variables in building the tree?\n",
    "  \n",
    "  f. Report if you see any evidence of model overfitting.\n",
    "  Nothing to indicate\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time:  14.181360006332397\n",
      "Train accuracy: 0.8306267070200044\n",
      "Test accuracy: 0.8292283844299001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89      8852\n",
      "           1       0.71      0.48      0.57      2760\n",
      "\n",
      "    accuracy                           0.83     11612\n",
      "   macro avg       0.78      0.71      0.73     11612\n",
      "weighted avg       0.82      0.83      0.82     11612\n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# record processing time taken\n",
    "startTime = time.time()\n",
    "# grid search CV\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 7),\n",
    "          'min_samples_leaf': range(200, 600, 100)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "print('processing time: ', time.time() - startTime)\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_decision_tree(cv.best_estimator_, X.columns, \"2.2tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the significant difference do you see between these two decision tree models – default (Task 2.1) and using GridSearchCV (Task 2.2)? How do theycompare performance-wise? Explain why those changes may have happened.\n",
    "\n",
    "The tree model in task 2.1, while faster to train and test, is far less accurate on the testing data than the training data and therefore likely overfitted. The GridSearchCV model produces far more consistent results between the training and testing data, however as a result takes much more proccessing time to achieve this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. From the better model, can you identify which householders to target for providing loan? Can you provide some descriptive summary of those householders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "## Predictive Modeling Using Regression (5.5 marks)\n",
    "1. Describe why you will have to do additional preparation for variables to be\n",
    "used in regression modelling. Apply transformation method(s) to the\n",
    "variable(s) that need it. List the variables that needed it.\n",
    "2. Build a regression model using the default regression method with all\n",
    "inputs. Once you have completed it, build another model and tune it usingGridSearchCV. Answer the followings:\n",
    "a. Report which variables are included in the regression model.\n",
    "b. Report the top-5 important variables (in the order) in the model.\n",
    "c. Report any sign of overfitting.\n",
    "d. What are the parameters used? Explain your decision. What are the\n",
    "optimal parameters? Which regression function is being used?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "3. Build another regression model using the subset of inputs selected either\n",
    "by RFE or the selection by model method. Answer the followings:\n",
    "a. Report which variables are included in the regression model.\n",
    "b. Report the top-5 important variables (in the order) in the model.\n",
    "c. Report any sign of overfitting.\n",
    "d. What is classification accuracy on training and test datasets?\n",
    "4. Using the comparison statistics, which of the regression models appears to\n",
    "be better? Is there any difference between the two models (i.e one with\n",
    "selected variables and another with all variables)? Explain why those\n",
    "changes may have happened.\n",
    "5. From the better model, can you identify which householders to target for\n",
    "providing loan? Can you provide some descriptive summary of those\n",
    "householders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## Predictive Modeling Using Neural Networks (5.5 marks)\n",
    "1. Build a Neural Network model using the default setting. Answer the\n",
    "following:\n",
    "a. What are the parameters used? Explain your decision. What is the\n",
    "network architecture?\n",
    "b. How many iterations are needed to train this network?\n",
    "c. Do you see any sign of over-fitting?\n",
    "d. Did the training process converge and resulted in the best model?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "2. Refine this network by tuning it with GridSearchCV. Answer the\n",
    "following:\n",
    "a. What are the parameters used? Explain your decision. What is the\n",
    "network architecture?\n",
    "b. How many iterations are needed to train this network?\n",
    "c. Do you see any sign of over-fitting?\n",
    "d. Did the training process converge and resulted in the best model?\n",
    "e. What is classification accuracy on training and test datasets?\n",
    "3. Would feature selection help here? Build another Neural Network model\n",
    "with inputs selected from RFE with regression (use the best model\n",
    "generated in Task 3) and from the decision tree (use the best model\n",
    "from Task 2). Answer the following for the best neural network model:a. Did feature selection help here? Which method of feature selection\n",
    "produced the best result? Any change in the network architecture?\n",
    "What inputs are being used as the network input?\n",
    "b. What is classification accuracy on training and test datasets? Is there\n",
    "any improvement in the outcome?\n",
    "c. How many iterations are now needed to train this network?\n",
    "d. Do you see any sign of over-fitting?\n",
    "e. Did the training process converge and resulted in the best model?\n",
    "f. Finally, see whether the change in network architecture can further\n",
    "improve the performance, use GridSearchCV to tune the network.\n",
    "Report if there was any improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "## Comparing Predictive Models (4 marks)\n",
    "1. Use the comparison methods to compare the best decision tree model, the\n",
    "best regression model, and the best neural network model.\n",
    "a. Discuss the findings led by:\n",
    "(i) ROC Chart and Index;\n",
    "(ii) Accuracy Score;\n",
    "b. Which model would you use in deployment based on these findings?\n",
    "Discuss why?\n",
    "c. Do all the models agree on the householder’s characteristics? How do\n",
    "they vary?\n",
    "2. How the outcome of this study can be used by decision makers?\n",
    "3. Can you summarise the positives and negative aspects of each predictive\n",
    "modelling method based on this data analysis exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
